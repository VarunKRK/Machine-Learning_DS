{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "\n",
    "It establishes the relationship between a categorical variable and one or more independent variables. This relationship is used in machine learning to predict the outcome of a categorical variable. It is widely used in many different fields such as the medical field, trading and business, technology, and many more.\n",
    "\n",
    "It is a classification algorithm used to find the probability of event success and event failure. It is used when the dependent variable is binary(0/1, True/False, Yes/No) in nature. It supports categorizing data into discrete classes by studying the relationship from a given set of labelled data. It learns a linear relationship from the given dataset and then introduces a non-linearity in the form of the Sigmoid function.\n",
    "\n",
    "### Adavantages \n",
    "- It is easy to implement yet provides great training efficiency in some cases. Training a model with this algorithm doesn't require high computation power.\n",
    "- In a low dimensional dataset having a sufficient number of training examples, logistic regression is less prone to over-fitting.\n",
    "- Logistic Regression proves to be very efficient when the dataset has features that are linearly separable.\n",
    "- In a low dimensional dataset having a sufficient number of training examples, logistic regression is less prone to over-fitting.\n",
    "\n",
    "### Disadvantages\n",
    "- Non linear problems can't be solved with logistic regression since it has a linear decision surface.\n",
    "- Only important and relevant features should be used to build a model otherwise the probabilistic predictions made by the model may be incorrect and the model's predictive value may degrade.\n",
    "- The presence of data values that deviate from the expected range in the dataset may lead to incorrect results as this algorithm is sensitive to outliers.\n",
    "- It is required that each training example be independent of all the other examples in the dataset. If they are related in some way, then the model will try to give more importance to those specific training examples. So, the training data should not come from matched data or repeated measurements. For example, some scientific research techniques rely on multiple observations on the same individuals. This technique can't be used in such cases.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Credit scoring\n",
    "- Medicine\n",
    "- Hotel booking\n",
    "- Emails: Spam or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, x, y):\n",
    "        self.intercept = np.ones((x.shape[0], 1))\n",
    "        self.x = np.concatenate((self.intercept, x), axis = 1)\n",
    "        self.weight = np.zeros(self.x.shape[1])\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "    '''Sigmoid method : The sigmoid function in logistic regression returns a probability value that can then be \n",
    "        mapped to two or more discrete classes. Given the set of input variables, our goal is to \n",
    "        assign that data point to a category (either 1 or 0). The sigmoid function outputs the \n",
    "        probability of the input points belonging to one of the classes.'''\n",
    "    def sigmoid(self, x, weight):\n",
    "        z = np.dot(x, weight)\n",
    "        return 1/(1 + np.exp(-z))\n",
    "\n",
    "\n",
    "\n",
    "    '''The Loss function : The loss function consists of parameters/weights, \n",
    "        when we say we want to optimize a loss function by this we simply refer to finding the best values of the\n",
    "        parameters/weights.''' \n",
    "    def loss(self, h, y):\n",
    "         l = (-y * np.log(h) - (1 - y) * np.log(1 - h))\n",
    "         return l.mean()\n",
    "\n",
    "\n",
    "\n",
    "    '''Gradient descent:  The Gradient descent is just the derivative of the loss function with respect to its weights.'''\n",
    "    def gd(self, X, h, y):\n",
    "        return np.dot(X.T, (h - y))/ y.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "    '''To implement the Algorithm we defined a fit method which requires the learning rate and the number of iterations as the input arguments.'''\n",
    "    def fit(self, lr, iterations):\n",
    "        for i in range(iterations):\n",
    "            sigma = self.sigmoid(self.x, self.weight)\n",
    "\n",
    "            loss = self.loss(sigma, self.y)\n",
    "\n",
    "            dW = self.gd(self.x, sigma, self.y)\n",
    "\n",
    "            #updating weights\n",
    "            self.weight -= lr*dW\n",
    "\n",
    "        return print('fitted successfully to data')\n",
    "\n",
    "\n",
    "\n",
    "    #Method to predict class label   \n",
    "    def predict(self, x_new, threshold):\n",
    "        x_new = np.concatenate((self.intercept, x_new), axis = 1)\n",
    "        result = self.sigmoid(x_new, self.weight)\n",
    "        result = result >= threshold\n",
    "        y_pred = np.zeros(result.shape[0])\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if result[i] == True:\n",
    "                y_pred[i] = 1\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\strive\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Anaconda\\envs\\strive\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Anaconda\\envs\\strive\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted successfully to data\n",
      "Accuracy is :  0.9261862917398945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "ds = load_breast_cancer()\n",
    "\n",
    "x = ds.data\n",
    "y = ds.target\n",
    "\n",
    "\n",
    "regressor = LogisticRegression(x, y)\n",
    "\n",
    "regressor.fit(0.01, 5000)\n",
    "\n",
    "\n",
    "y_pred = regressor.predict(x, 0.5)\n",
    "\n",
    "\n",
    "accuracy = sum(y_pred == y)/y.shape[0]\n",
    "\n",
    "print('Accuracy is : ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e98eea00a698c66ff9007bd5a8bf6209cdaa78d416205c57d24ef9296d8223a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('strive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
